{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33931c52",
   "metadata": {},
   "source": [
    "# Battery-included Design mocks for AIoD workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3119c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Workflow 1: Model Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79500067",
   "metadata": {},
   "source": [
    "Users of AIoD should be able to get models from popular machine learning packages. AIoD will thus become a common interface for all the popular ML libraries that are indexed by it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec247fc7",
   "metadata": {},
   "source": [
    "### Workflow 1a: Retrieving classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a88e9b",
   "metadata": {},
   "source": [
    "By using `aiod.get()`, users can directly import any class from any library that is indexed by AIoD. If the required soft deoendencies are present in the environment (e.g. `scikit-learn`, `xgboost`, `sktime`, `mlxtend`, `pytorch-tabular`, etc.), then the classes will be imported otherwise an error will be raised to let users know of the missing soft dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ac872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "\n",
    "RandomForestClassifier = aiod.get(\"RandomForestClassifier\")\n",
    "XGBClassifier = aiod.get(\"XGBClassifier\")\n",
    "LGBMClassifier = aiod.get(\"LGBMClassifier\")\n",
    "NaiveForecaster = aiod.get(\"NaiveForecaster\")\n",
    "EnsembleVoteClassifier = aiod.get(\"EnsembleVoteClassifier\")\n",
    "SimpleImputer = aiod.get(\"SimpleImputer\")\n",
    "OneHotEncoder = aiod.get(\"OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47496481",
   "metadata": {},
   "source": [
    "So in the above example,\n",
    "\n",
    "`RandomForestClassifier = aiod.get(\"RandomForestClassifier\")` would be same as `from sklearn.ensemble import RandomForestClassifier` and `print(type(RandomForestClassifier))` would return `<class 'type'>`\n",
    "\n",
    "and so will the other examples. This will turn AIoD into an ML algorithms index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54112af8",
   "metadata": {},
   "source": [
    "### Workflow 1b: Retrieving instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290dc1a4",
   "metadata": {},
   "source": [
    "Besides classes, users should also be able to retrieve live instances of the class. These instances can be\n",
    "\n",
    "* an estimator instance without any hyperparams\n",
    "\n",
    "* an estimator instance with hyperparams\n",
    "\n",
    "* a preprocessing step\n",
    "\n",
    "* a pipeline\n",
    "\n",
    "etc.\n",
    "\n",
    "This would be useful in getting the exact instance used in an experiment. More on this in Workflow 2 and Workflow 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiod import craft\n",
    "\n",
    "rf_classifier = craft(\"RandomForestClassifier(n_estimators=100)\")\n",
    "pipeline = craft(\"Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), ('classifier', RandomForestClassifier(n_estimators=100))])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273276c",
   "metadata": {},
   "source": [
    "`print(type(rf_classifier))` would then return `<class 'sklearn.ensemble._forest.RandomForestClassifier'>`.\n",
    "\n",
    "A user can now directly fit an instantiated object. In the below example, we will see how a user can use the `pipeline` built in the above example from a string specification using `craft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7991c82",
   "metadata": {},
   "source": [
    "Notice the difference between AIoD and HuggingFace from the above examples. We are dealing with classes and instances and not the model weights, but HuggingFace deals with model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab967c",
   "metadata": {},
   "source": [
    "## Workflow 2: Model Catalogues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c891b",
   "metadata": {},
   "source": [
    "A catalogue is a curated collection of machine learning components. These components can be estimators, datasets, and metrics. For now, we will limit our scope to model (estimator) catalogues. But catalogues can be of mixed type too, representing an entire benchmark setup, more on this in Workflow 3.\n",
    "\n",
    "Let's say there is a popular benchmarking paper from NeurIPS which compares different tabular classification models. A catalogue, then, allows to create a collection of all the models used in the paper with or without hyperparams as used in the paper, so that a user can get them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiod.catalogues import NeurIPS2026ClassificationCatalogue\n",
    "\n",
    "catalogue = NeurIPS2026ClassificationCatalogue()\n",
    "\n",
    "# returns a list of all estimators in the catalogue as strings\n",
    "print(catalogue.get(object_type=\"all\"))\n",
    "\n",
    "# returns a list of all estimators in the catalogue as instantiated objects;\n",
    "# passing `as_object=True` internally calls `craft` on each of the strings\n",
    "# and instantiates them as estimator instances (see workflow 1b above)\n",
    "print(catalogue.get(object_type=\"all\", as_object=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d2f03",
   "metadata": {},
   "source": [
    "## Workflow 3: Model Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096620dc",
   "metadata": {},
   "source": [
    "We will now see how Workflow 1 and Workflow 2 enable us to carry efficient benchmarking experiments using AIoD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0be52e",
   "metadata": {},
   "source": [
    "### Workflow 3a: Basic Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2269557",
   "metadata": {},
   "source": [
    "Users should be able to import models from different machine learning libraries, register them with the benchmark, define one or more tasks (including dataset loaders, resampling strategies, and evaluation metrics), and then execute the benchmark with a single, consistent interface. The system should handle fitting, prediction, scoring, and timing automatically across all specified configurations.\n",
    "\n",
    "Upon execution, the benchmark should return a structured dataframe containing the aggregated results of the experiment. This dataframe should summarize predictive performance (e.g., metric values per fold, means, and standard deviations) as well as computational statistics such as fit time, prediction time, and total runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d40629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "from aiod.benchmarking import ClassificationBenchmark\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "RandomForestClassifier = aiod.get(\"RandomForestClassifier\")\n",
    "XGBClassifier = aiod.get(\"XGBClassifier\")\n",
    "LGBMClassifier = aiod.get(\"LGBMClassifier\")\n",
    "\n",
    "benchmark = ClassificationBenchmark()\n",
    "\n",
    "benchmark.add(RandomForestClassifier(n_estimators=100))\n",
    "benchmark.add(XGBClassifier(n_estimators=100))\n",
    "benchmark.add(LGBMClassifier(n_estimators=100))\n",
    "\n",
    "benchmark.add(load_iris(return_X_y=True))\n",
    "benchmark.add(KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "benchmark.add(accuracy_score)\n",
    "\n",
    "results = benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496d853",
   "metadata": {},
   "source": [
    "### Workflow 3b: Reproducing and Extending Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4ec76",
   "metadata": {},
   "source": [
    "In the above example, we added a bunch of estimators and a bunch of tasks and ran the benchmark. But a user should be able to add all the estimators from an existing experiment (e.g. a NeurIPS paper) at once, without writing the boilerplate code. The benchmark object should internally get the estimators from catalogues and add them to itself for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56008c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiod.benchmarking import ClassificationBenchmark\n",
    "from aiod.catalogues import NeurIPS2026ClassificationCatalogue\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "benchmark = ClassificationBenchmark()\n",
    "catalogue = NeurIPS2026ClassificationCatalogue()\n",
    "\n",
    "# adds all the estimators from the catalogue (reproduce the experiment)\n",
    "benchmark.add(catalogue)\n",
    "\n",
    "# add another estimaator (extend the experiment)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "benchmark.add(LogisticRegression())\n",
    "\n",
    "benchmark.add(load_iris(return_X_y=True))\n",
    "benchmark.add(KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "benchmark.add(accuracy_score)\n",
    "\n",
    "benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e138be",
   "metadata": {},
   "source": [
    "In the above example, and in Workflow 2, we had a catalogue which contained just the estimators. But there can be catalogues of mixed object types as well, containing estimators, dataset loaders, metrics, and cv splitters. In that case we can directly add the catalogue to the benchmark, and the benchmark internally resolves the catalogue and identifies the estimators and tasks, adding them to itself and running the benchmark as demonstrated in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiod.benchmarking import ClassificationBenchmark\n",
    "from aiod.catalogues import NeurIPS2026ClassificationCatalogueonSteroids\n",
    "\n",
    "benchmark = ClassificationBenchmark()\n",
    "catalogue = NeurIPS2026ClassificationCatalogueonSteroids()\n",
    "\n",
    "# adds all the estimators from the catalogue (reproduce the experiment)\n",
    "benchmark.add(catalogue)\n",
    "\n",
    "# add another estimaator (extend the experiment)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "benchmark.add(LogisticRegression())\n",
    "\n",
    "benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e87329",
   "metadata": {},
   "source": [
    "## Workflow 4: Getting Models from Scientific Papers/Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cdc1f",
   "metadata": {},
   "source": [
    "WIP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
