{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33931c52",
   "metadata": {},
   "source": [
    "# Design Mockups for AIoD Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3119c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Workflow 1: Model Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79500067",
   "metadata": {},
   "source": [
    "Users of AIoD should be able to get models from popular machine learning packages. AIoD will thus become a common interface for all the popular ML libraries that are indexed by it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec247fc7",
   "metadata": {},
   "source": [
    "### Workflow 1a: Retrieving classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a88e9b",
   "metadata": {},
   "source": [
    "By using `aiod.get()`, users can directly import any class from any library that is indexed by AIoD. If the required soft deoendencies are present in the environment (e.g. `scikit-learn`, `xgboost`, `sktime`, `mlxtend`, `pytorch-tabular`, etc.), then the classes will be imported otherwise an error will be raised to let users know of the missing soft dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ac872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "\n",
    "RandomForestClassifier = aiod.get(\"RandomForestClassifier\")\n",
    "XGBClassifier = aiod.get(\"XGBClassifier\")\n",
    "LGBMClassifier = aiod.get(\"LGBMClassifier\")\n",
    "NaiveForecaster = aiod.get(\"NaiveForecaster\")\n",
    "EnsembleVoteClassifier = aiod.get(\"EnsembleVoteClassifier\")\n",
    "SimpleImputer = aiod.get(\"SimpleImputer\")\n",
    "OneHotEncoder = aiod.get(\"OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47496481",
   "metadata": {},
   "source": [
    "So in the above example,\n",
    "\n",
    "`RandomForestClassifier = aiod.get(\"RandomForestClassifier\")` would be same as `from sklearn.ensemble import RandomForestClassifier` and `print(type(RandomForestClassifier))` would return `<class 'type'>`\n",
    "\n",
    "and so will the other examples. This will turn AIoD into an ML algorithms index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54112af8",
   "metadata": {},
   "source": [
    "### Workflow 1b: Retrieving instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290dc1a4",
   "metadata": {},
   "source": [
    "Besides classes, users should also be able to retrieve live instances of the class. These instances can be\n",
    "\n",
    "* an estimator instance without any hyperparams\n",
    "\n",
    "* an estimator instance with hyperparams\n",
    "\n",
    "* a preprocessing step\n",
    "\n",
    "* a pipeline\n",
    "\n",
    "etc.\n",
    "\n",
    "This would be useful in getting the exact instance used in an experiment. More on this in Workflow 2 and Workflow 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "\n",
    "rf_classifier = aiod.get(\"RandomForestClassifier(n_estimators=100)\")\n",
    "pipeline = aiod.get(\"Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), ('classifier', RandomForestClassifier(n_estimators=100))])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273276c",
   "metadata": {},
   "source": [
    "`print(type(rf_classifier))` would then return `<class 'sklearn.ensemble._forest.RandomForestClassifier'>`.\n",
    "\n",
    "A user can now directly fit an instantiated object. In the below example, we will see how a user can use the `pipeline` built in the above example from a string specification using `craft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7991c82",
   "metadata": {},
   "source": [
    "Notice the difference between AIoD and HuggingFace from the above examples. We are dealing with classes and instances and not the model weights, but HuggingFace deals with model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30acb54b",
   "metadata": {},
   "source": [
    "### Workflow 1c: Executable specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209a804",
   "metadata": {},
   "source": [
    "Beyond simple class lookup and instance construction, users should be able to use `aiod.get()` to construct fully executable multi-line specifications. These specifications may define intermediate variables and must end with a return statement indicating the object to be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "\n",
    "spec = \"\"\"\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))])\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "return GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=[{\n",
    "        \"classifier__max_depth\": [5, 10],\n",
    "        \"classifier__min_samples_split\": [2, 5],\n",
    "    },\n",
    "    ],\n",
    "    cv=cv,\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "print(aiod.get(spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f137db",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n",
    "             estimator=Pipeline(steps=[('imputer', SimpleImputer()),\n",
    "                                       ('scaler', StandardScaler()),\n",
    "                                       ('classifier',\n",
    "                                        RandomForestClassifier())]),\n",
    "             param_grid=[{'classifier__max_depth': [5, 10],\n",
    "                          'classifier__min_samples_split': [2, 5]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab967c",
   "metadata": {},
   "source": [
    "## Workflow 2: Model Catalogues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c891b",
   "metadata": {},
   "source": [
    "A catalogue is a curated collection of machine learning components. These components can be estimators, datasets, and metrics. For now, we will limit our scope to model (estimator) catalogues. But catalogues can be of mixed type too, representing an entire benchmark setup, more on this in Workflow 3.\n",
    "\n",
    "Let's say there is a popular benchmarking paper from NeurIPS which compares different tabular classification models. A catalogue, then, allows to create a collection of all the models used in the paper with or without hyperparams as used in the paper, so that a user can get them all at once. For this below example, we will assume that this catalogue contains three classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc5756",
   "metadata": {},
   "source": [
    "Returns a list of all estimators in the catalogue as strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = aiod.get(\"NeurIPS2026ClassificationCatalogue()\")\n",
    "\n",
    "print(catalogue.fetch(object_type=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402945b",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    \"RandomForestClassifier(n_estimators=100)\", \n",
    "    \"XGBClassifier(n_estimators=100)\", \n",
    "    \"LGBMClassifier(n_estimators=100)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84f4be",
   "metadata": {},
   "source": [
    "Returns a list of all estimators in the catalogue as instantiated objects; passing `as_object=True` internally calls `craft` on each of the strings and instantiates them as estimator instances (see workflow 1b above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalogue.fetch(object_type=\"all\", as_object=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d09953",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c81c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    RandomForestClassifier(n_estimators=100), \n",
    "    XGBClassifier(n_estimators=100), \n",
    "    LGBMClassifier(n_estimators=100),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d2f03",
   "metadata": {},
   "source": [
    "## Workflow 3: Model Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096620dc",
   "metadata": {},
   "source": [
    "We will now see how Workflow 1 and Workflow 2 enable us to carry efficient benchmarking experiments using AIoD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0be52e",
   "metadata": {},
   "source": [
    "### Workflow 3a: Basic Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2269557",
   "metadata": {},
   "source": [
    "Users should be able to register estimator instances with the benchmark, define one or more tasks (including dataset loaders, resampling strategies, and evaluation metrics), and then execute the benchmark with a single, consistent interface. The system should handle fitting, prediction, scoring, and timing automatically across all specified configurations.\n",
    "\n",
    "Upon execution, the benchmark should return a structured dataframe containing the aggregated results of the experiment as a leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d40629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiod.benchmarking import ClassificationBenchmark\n",
    "\n",
    "benchmark = ClassificationBenchmark()\n",
    "\n",
    "benchmark.add(\"RandomForestClassifier(n_estimators=100)\")\n",
    "benchmark.add(\"XGBClassifier(n_estimators=100)\")\n",
    "benchmark.add(\"LGBMClassifier(n_estimators=100)\")\n",
    "\n",
    "benchmark.add(\"load_iris(return_X_y=True)\")\n",
    "benchmark.add(\"KFold(n_splits=2, shuffle=True, random_state=42)\")\n",
    "benchmark.add(\"accuracy_score\")\n",
    "\n",
    "results = benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72d2ca",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23849f",
   "metadata": {},
   "source": [
    "| Model                  | Organization/Library | Accuracy | Accuracy Rank |\n",
    "|------------------------|--------------|----------|---------------|\n",
    "| RandomForestClassifier | scikit-learn   | 0.9733   | 1             |\n",
    "| XGBClassifier          | xgboost | 0.9533   | 2             |\n",
    "| LGBMClassifier         | lightgbm    | 0.9467   | 3             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496d853",
   "metadata": {},
   "source": [
    "### Workflow 3b: Reproducing and Extending Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4ec76",
   "metadata": {},
   "source": [
    "In the above example, we added a bunch of estimators and a bunch of tasks and ran the benchmark. But a user should be able to add all the estimators from an existing experiment (e.g. a NeurIPS paper) at once, without writing the boilerplate code. The benchmark object should internally get the estimators from catalogues and add them to itself for execution. Users should also be able to extend the benchmark experiment by adding estimators besides what are contained in a catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56008c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "from aiod.benchmarking import ClassificationBenchmark\n",
    "\n",
    "benchmark = ClassificationBenchmark()\n",
    "catalogue = aiod.get(\"NeurIPS2026ClassificationCatalogue()\")\n",
    "\n",
    "# adds all the estimators from the catalogue (reproduce the experiment)\n",
    "benchmark.add(catalogue)\n",
    "\n",
    "# add another estimaator (extend the experiment)\n",
    "benchmark.add(\"LogisticRegression()\")\n",
    "\n",
    "# add tasks\n",
    "benchmark.add(\"load_iris(return_X_y=True)\")\n",
    "benchmark.add(\"KFold(n_splits=2, shuffle=True, random_state=42)\")\n",
    "benchmark.add(\"accuracy_score\")\n",
    "\n",
    "benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f79791",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa8a5d",
   "metadata": {},
   "source": [
    "| Model                  | Organization/Library | Accuracy | Accuracy Rank |\n",
    "|------------------------|--------------|----------|---------------|\n",
    "| RandomForestClassifier | scikit-learn   | 0.9733   | 1             |\n",
    "| XGBClassifier          | xgboost | 0.9533   | 2             |\n",
    "| LGBMClassifier         | lightgbm    | 0.9467   | 3             |\n",
    "| LogisticRegression     | scikit-learn | 0.9343 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ed9d5",
   "metadata": {},
   "source": [
    "Notice that we also have a fourth column now for `LogisticRegression`, that we added on top of the estimators from the catalogues, so we can see how _our_ added estimator/algorithm performs as compared to the algorithms in a given catalogue (or e.g. a NeurIPS paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e138be",
   "metadata": {},
   "source": [
    "In the above example, and in Workflow 2, we had a catalogue which contained just the estimators. But there can be catalogues of mixed object types as well, containing estimators, dataset loaders, metrics, and cv splitters. In that case we can directly add the catalogue to the benchmark, and the benchmark internally resolves the catalogue and identifies the estimators and tasks, adding them to itself and running the benchmark as demonstrated in the example below. Let's assume in the below example, `NeurIPSClassificationCatalogueonSteroids` is a catalogue of mixed object type and contains estimators, dataset loaders, metrics, and cv splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiod\n",
    "from aiod.benchmarking import ClassificationBenchmark\n",
    "\n",
    "benchmark = ClassificationBenchmark()\n",
    "\n",
    "# adds all the estimators from the catalogue (reproduce the experiment)\n",
    "benchmark.add(\"NeurIPS2026ClassificationCatalogueonSteroids()\")\n",
    "\n",
    "# add another estimaator (extend the experiment)\n",
    "benchmark.add(\"LogisticRegression()\")\n",
    "\n",
    "benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812fcac5",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267019f",
   "metadata": {},
   "source": [
    "| Model                  | Organization/Library | Accuracy | Accuracy Rank |\n",
    "|------------------------|--------------|----------|---------------|\n",
    "| RandomForestClassifier | scikit-learn   | 0.9733   | 1             |\n",
    "| XGBClassifier          | xgboost | 0.9533   | 2             |\n",
    "| LGBMClassifier         | lightgbm    | 0.9467   | 3             |\n",
    "| LogisticRegression     | scikit-learn | 0.9343 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fa1d4",
   "metadata": {},
   "source": [
    "Since in `NeurIPSClassificationCatalogueonSteroids`, the tasks were also included in the catalogue, we did not have to add them seperately to the benchmark object in the above example. It got added automatically via the catalogue, and results in the same result dataframe as the example before where we added tasks on top of adding the estimator catalogue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e87329",
   "metadata": {},
   "source": [
    "## Workflow 4: Getting Models from Scientific Papers/Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cdc1f",
   "metadata": {},
   "source": [
    "WIP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
